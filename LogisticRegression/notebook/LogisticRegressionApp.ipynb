{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### FPGA-ACCELERATION OF MACHINE LEARNING IN CLOUD COMPUTING, A CASE STUDY USING LOGISTIC REGRESSION\n",
    "\n",
    "<img style=\"float: left;\" src=\"data/sample.png\">\n",
    "\n",
    "# Classifying Handwritten Digits with <br /> Logistic Regression <br />\n",
    "\n",
    "___\n",
    "\n",
    "This notebook demonstrates how to interface to our hardware ML library from Python, while also explains how the communication between Python and the hardware accelerator is performed. The Logistic Regression (LR) overlay that is used has been created with an accelerator (LR_gradients_kernel), that receives data from Python, processes it, and returns the results, using AXI4-Stream Accelerator Adapter.\n",
    "\n",
    "## 1. Accelerator API\n",
    "\n",
    "First, we download the LR overlay onto the device and we map the DMAs of the hardware accelerator to DMA\n",
    "objects, using Xilinxâ€™s built-in modules and classes. We then allocate the corresponding buffers for the DMAs and after writing the input data (data1, data2, weights) to them, we can finally transfer them to the accelerator.  We also send the size of the data chunk along with the necessary commands to the Accelerator Adapter. Gradients are computed in return."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python      \n",
    "from pynq import MMIO, Overlay, PL\n",
    "from pynq.mllib_accel import DMA\n",
    "\n",
    "DMA_TO_DEV = 0    # DMA sends data to PL.\n",
    "DMA_FROM_DEV = 1  # DMA receives data from PL.\n",
    "\n",
    "class LR_Accel:\n",
    "    \"\"\"\n",
    "    Python class for the LR Accelerator.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, chunkSize, numClasses, numFeatures):\n",
    "        self.numClasses = numClasses\n",
    "        self.numFeatures = numFeatures\n",
    "           \n",
    "        # -------------------------\n",
    "        #   Download Overlay.\n",
    "        # -------------------------    \n",
    "\n",
    "        ol = Overlay(\"LogisticRegression.bit\")\n",
    "        ol.download()  \n",
    "        \n",
    "        # -------------------------\n",
    "        #   Physical address of the Accelerator Adapter IP.\n",
    "        # -------------------------\n",
    "\n",
    "        ADDR_Accelerator_Adapter_BASE = int(PL.ip_dict[\"SEG_LR_gradients_kernel_accel_0_if_Reg\"][0], 16)\n",
    "        ADDR_Accelerator_Adapter_RANGE = int(PL.ip_dict[\"SEG_LR_gradients_kernel_accel_0_if_Reg\"][1], 16)\n",
    "\n",
    "        # -------------------------\n",
    "        #    Initialize new MMIO object. \n",
    "        # -------------------------\n",
    "\n",
    "        self.bus = MMIO(ADDR_Accelerator_Adapter_BASE, ADDR_Accelerator_Adapter_RANGE)\n",
    "\n",
    "        # -------------------------\n",
    "        #   Physical addresses of the DMA IPs.\n",
    "        # -------------------------\n",
    "\n",
    "        ADDR_DMA0_BASE = int(PL.ip_dict[\"SEG_dm_0_Reg\"][0], 16)\n",
    "        ADDR_DMA1_BASE = int(PL.ip_dict[\"SEG_dm_1_Reg\"][0], 16)\n",
    "        ADDR_DMA2_BASE = int(PL.ip_dict[\"SEG_dm_2_Reg\"][0], 16)\n",
    "        ADDR_DMA3_BASE = int(PL.ip_dict[\"SEG_dm_3_Reg\"][0], 16)\n",
    "\n",
    "        # -------------------------\n",
    "        #    Initialize new DMA objects. \n",
    "        # -------------------------\n",
    "\n",
    "        self.dma0 = DMA(ADDR_DMA0_BASE, direction = DMA_TO_DEV)    # data1 DMA.\n",
    "        self.dma1 = DMA(ADDR_DMA1_BASE, direction = DMA_TO_DEV)    # data2 DMA.\n",
    "        self.dma2 = DMA(ADDR_DMA2_BASE, direction = DMA_TO_DEV)    # weights DMA.\n",
    "        self.dma3 = DMA(ADDR_DMA3_BASE, direction = DMA_FROM_DEV)  # gradients DMA.\n",
    "        \n",
    "        # -------------------------\n",
    "        #    Allocate physically contiguous memory buffers.\n",
    "        # -------------------------\n",
    "\n",
    "        self.dma0.create_buf(int(chunkSize / 2) * (self.numClasses + (1 + self.numFeatures)) * 4, 1)\n",
    "        self.dma1.create_buf(int(chunkSize / 2) * (self.numClasses + (1 + self.numFeatures)) * 4, 1)\n",
    "        self.dma2.create_buf((self.numClasses * (1 + self.numFeatures)) * 4, 1)\n",
    "        self.dma3.create_buf((self.numClasses * (1 + self.numFeatures)) * 4, 1)\n",
    "\n",
    "        # -------------------------\n",
    "        #    Get CFFI pointers to objects' internal buffers.\n",
    "        # -------------------------\n",
    "\n",
    "        self.data1_buf = self.dma0.get_buf(32, data_type = \"float\")\n",
    "        self.data2_buf = self.dma1.get_buf(32, data_type = \"float\")\n",
    "        self.weights_buf = self.dma2.get_buf(32, data_type = \"float\")\n",
    "        self.gradients_buf = self.dma3.get_buf(32, data_type = \"float\")\n",
    "\n",
    "    def gradients_kernel(self, data, weights):\n",
    "        chunkSize = int(len(data) / (self.numClasses + (1 + self.numFeatures)))\n",
    "        \n",
    "        for i in range (0, int(len(data) / 2)):\n",
    "            self.data1_buf[i] = float(data[i])\n",
    "            self.data2_buf[i] = float(data[int(len(data) / 2) + i])\n",
    "        for kj in range (0, self.numClasses * (1 + self.numFeatures)):\n",
    "            self.weights_buf[kj] = float(weights[kj])\n",
    "\n",
    "        # -------------------------\n",
    "        #   Write data to MMIO.\n",
    "        # -------------------------\n",
    "\n",
    "        CMD = 0x0028            # Command.\n",
    "        ISCALAR0_DATA = 0x0080  # Input Scalar-0 Write Data FIFO.\n",
    "\n",
    "        self.bus.write(ISCALAR0_DATA, int(chunkSize))\n",
    "        self.bus.write(CMD, 0x00010001)\n",
    "        self.bus.write(CMD, 0x00020000)\n",
    "        self.bus.write(CMD, 0x00000107)\n",
    "\n",
    "        # -------------------------\n",
    "        #   Transfer data using DMAs (Non-blocking).\n",
    "        #   Block while DMAs are busy.\n",
    "        # -------------------------\n",
    "\n",
    "        self.dma0.transfer(int(len(data) / 2) * 4, direction = DMA_TO_DEV)\n",
    "        self.dma1.transfer(int(len(data) / 2) * 4, direction = DMA_TO_DEV)\n",
    "        self.dma2.transfer((self.numClasses * (1 + self.numFeatures)) * 4, direction = DMA_TO_DEV)\n",
    "\n",
    "        self.dma0.wait()\n",
    "        self.dma1.wait()\n",
    "        self.dma2.wait()\n",
    "\n",
    "        self.dma3.transfer((self.numClasses * (1 + self.numFeatures)) * 4, direction = DMA_FROM_DEV)\n",
    "\n",
    "        self.dma3.wait()\n",
    "\n",
    "        gradients = []\n",
    "        for kj in range (0, self.numClasses * (1 + self.numFeatures)):\n",
    "            gradients.append(float(self.gradients_buf[kj]))\n",
    "\n",
    "        return gradients\n",
    "    \n",
    "    def __del__(self):\n",
    "\n",
    "        # -------------------------\n",
    "        #   Destructors for DMA objects.\n",
    "        # -------------------------\n",
    "\n",
    "        self.dma0.__del__()\n",
    "        self.dma1.__del__()\n",
    "        self.dma2.__del__()\n",
    "        self.dma3.__del__()\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Set Introduction\n",
    "\n",
    "The data are taken from the famous <a href=\"http://yann.lecun.com/exdb/mnist/\">MNIST</a> dataset. \n",
    "\n",
    "The original data file contains gray-scale images of hand-drawn digits, from zero through nine. Each image is 28 pixels in height and 28 pixels in width, for a total of 784 pixels in total. Each pixel has a single pixel-value associated with it, indicating the lightness or darkness of that pixel, with higher numbers meaning darker. This pixel-value is an integer between 0 and 255, inclusive.\n",
    "\n",
    "In this example the data we use are already preprocessed/normalized using Feature Standardization method (<a href=\"https://en.wikipedia.org/wiki/Standard_score\">Z-score scaling</a>).\n",
    "\n",
    "### Export data sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train.dat\n",
      "test.dat\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "print(os.popen(\"tar -zxvf data/datasets_MNIST_small.tar.gz -C data/\").read().strip(\"\\n\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data sets (train, test) have 785 columns. The first column, called \"label\", is the digit that was drawn by the user. The rest of the columns contain the (rescaled) pixel-values of the associated image.\n",
    "\n",
    "## 3. Logistic Regression Application\n",
    "\n",
    "This example shows how our Logistic Regression library is called to train a LR model on the train set and then test its accuracy. If \\_accel\\_ = 1, the hardware accelerator is used for the computation of the gradients in each iteration, through the driver we introduced above (LR_Accel).\n",
    "\n",
    "### Read data & parameters\n",
    "\n",
    "The size of the train set, as well as the number of the iterations are intentionally picked small, to avoid large execution time in SW-only cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* LogisticRegression Application *\n",
      " # train file:               data/train.dat\n",
      " # test file:                data/test.dat\n"
     ]
    }
   ],
   "source": [
    "# Data sets\n",
    "train_file = \"data/train.dat\"\n",
    "test_file = \"data/test.dat\"\n",
    "\n",
    "print(\"* LogisticRegression Application *\")\n",
    "print(\" # train file:               \" + train_file)\n",
    "print(\" # test file:                \" + test_file)\n",
    "\n",
    "# Read train file\n",
    "trainFile = []  \n",
    "with open(train_file, \"r\") as lines:\n",
    "    for line in lines:\n",
    "        trainFile.append(line.strip(\"\\n\"))      \n",
    "lines.close()\n",
    "     \n",
    "# Read test file\n",
    "testFile = []  \n",
    "with open(test_file, \"r\") as lines:\n",
    "    for line in lines:\n",
    "        testFile.append(line.strip(\"\\n\"))       \n",
    "lines.close()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Select mode: (0: SW-only, 1: HW accelerated)\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "chunkSize = 4000\n",
    "alpha = 0.25    # learning rate\n",
    "iterations = 3  # number of iterations\n",
    "print(\"Select mode: (0: SW-only, 1: HW accelerated)\")\n",
    "_accel_ = int(input()) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instantiate a Logistic Regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pynq.mllib_accel.classification import LogisticRegression\n",
    "\n",
    "numClasses = 10  \n",
    "numFeatures = 784 \n",
    "LR = LogisticRegression(numClasses, numFeatures)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the LR model    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    * LogisticRegression Training *\n",
      "     # numSamples:               12000\n",
      "     # chunkSize:                4000\n",
      "     # numClasses:               10\n",
      "     # numFeatures:              784\n",
      "     # alpha:                    0.25\n",
      "     # iterations:               3\n",
      "! Time running training in hardware: 154.647 sec\n"
     ]
    }
   ],
   "source": [
    "weights = LR.train(trainFile, chunkSize, alpha, iterations, _accel_)\n",
    "    \n",
    "# Write weights file    \n",
    "with open(\"data/weights.out\", \"w\") as weights_file:\n",
    "    for k in range(0, numClasses):\n",
    "        for j in range(0, (1 + numFeatures)):\n",
    "            if j == 0:\n",
    "                weights_file.write(str(round(weights[k * (1 + numFeatures) + j], 5)))\n",
    "            else:\n",
    "                weights_file.write(\",\" + str(round(weights[k * (1 + numFeatures) + j], 5)))\n",
    "        weights_file.write(\"\\n\")\n",
    "weights_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the LR model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    * LogisticRegression Testing *\n",
      "     # accuracy:                 0.8155(1631/2000)\n",
      "     # true:                     1631\n",
      "     # false:                    369\n"
     ]
    }
   ],
   "source": [
    "LR.test(testFile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 4. Performance\n",
    "\n",
    "We present execution time measurements in different target devices.\n",
    "\n",
    "`! Time running training in`\n",
    "\n",
    "Target | Time\n",
    ":--- | ---:\n",
    "`PYNQ SW-only:` | `2648.151 sec` \n",
    "`Intel Core i5-5200U @ 2.20GHz:` | `235.819 sec`\n",
    "`PYNQ HW accelerated:` | `154.647 sec`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
